{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Net 是一种经典的卷积神经网络（CNN）架构，广泛应用于图像分割任务，特别是医学图像分割。它由德国弗赖堡大学的研究人员在2015年提出，目的是提高少量标注数据下的分割效果。U-Net 得名于其类似字母“U”形的结构，具有一个下采样的编码器部分和一个上采样的解码器部分，确保了网络能够从细节到全局地提取特征。\n",
    "\n",
    "### U-Net 结构概述\n",
    "\n",
    "1. **编码器（下采样）**：\n",
    "   - 编码器部分通常由卷积层、ReLU 激活函数和池化层组成。它通过逐步缩小输入图像的空间尺寸来提取高层次特征（即提取图像的全局语义信息）。\n",
    "   - 每经过一次池化操作，特征图的尺寸会减小一半，但特征的维度通常会加倍，这样可以在更深的网络层次上获得更丰富的特征表示。\n",
    "\n",
    "2. **瓶颈层**：\n",
    "   - 编码器部分和解码器部分之间是一个瓶颈层，通常包含一系列卷积操作，用于在最小的空间维度下提取高层次的抽象特征。\n",
    "\n",
    "3. **解码器（上采样）**：\n",
    "   - 解码器通过上采样（或反卷积）操作逐步恢复图像的空间分辨率。它的作用是根据编码器提取的特征图恢复图像的精细结构。\n",
    "   - 解码器部分的每一层通常都会与对应编码器部分的特征图进行跳跃连接（skip connections）。\n",
    "\n",
    "4. **跳跃连接（Skip Connections）**：\n",
    "   - 跳跃连接是 U-Net 的核心特色之一。在解码器的每一层中，都会将编码器中相同分辨率的特征图与当前层的特征图进行拼接。这使得网络能够结合高分辨率的局部特征和低分辨率的语义特征，从而提升分割精度。\n",
    "\n",
    "5. **输出层**：\n",
    "   - 输出层通常是一个 1x1 的卷积层，用于将最终的特征图映射到所需的类别数（对于二分类问题，通常是1个通道）。\n",
    "\n",
    "### 优点\n",
    "\n",
    "- **精确的边界捕捉**：跳跃连接使得 U-Net 能够有效地捕捉到图像中的细节和边界信息，尤其适用于医学图像分割等精细任务。\n",
    "- **少量数据训练**：U-Net 可以在相对较少的标注数据下训练得比较好，这是因为它通过跳跃连接利用了大量局部信息，减少了对大规模数据集的依赖。\n",
    "- **端到端训练**：U-Net 可以进行端到端训练，即输入图像和目标分割图像直接作为训练数据。\n",
    "\n",
    "### 应用\n",
    "\n",
    "- **医学图像分割**：U-Net 在医学图像处理（如器官分割、肿瘤分割等）方面表现出色，能够从复杂的医学图像中提取精确的区域。\n",
    "- **卫星图像分析**：U-Net 也常用于卫星图像中的地物分类或土地覆盖分类任务。\n",
    "- **遥感图像分割**：同样，U-Net 在遥感领域用于地物识别和分割。\n",
    "\n",
    "### 变体和改进\n",
    "\n",
    "随着 U-Net 在不同任务中的应用，研究者提出了多种 U-Net 的改进版本，包括：\n",
    "\n",
    "- **3D U-Net**：用于处理三维数据（如CT扫描），其结构类似于2D U-Net，但卷积和池化操作扩展到三维。\n",
    "- **Attention U-Net**：引入了注意力机制，可以让网络关注图像中更重要的区域。\n",
    "- **ResUNet**：结合了残差连接的 U-Net，有助于解决深层网络训练时的梯度消失问题。\n",
    "\n",
    "总的来说，U-Net 是一种非常有效的图像分割架构，特别是在数据集相对较小的情况下，它能够通过其独特的结构和设计在许多领域取得优秀的分割性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 1484/1484 [02:02<00:00, 12.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.20093649298152222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 1484/1484 [02:01<00:00, 12.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.13500404681252928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 1484/1484 [02:02<00:00, 12.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.128820448617904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 1484/1484 [02:04<00:00, 11.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.12437926482213435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:  10%|▉         | 145/1484 [00:11<01:41, 13.16it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 123\u001b[0m\n\u001b[1;32m    120\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[47], line 103\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    101\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 103\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # 编码器部分（Encoder）\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # 中间部分（Middle）\n",
    "        self.middle = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # 解码器部分（Decoder）\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2),  # 上采样\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, out_channels, kernel_size=2, stride=2)  # 再次上采样\n",
    "        )\n",
    "        \n",
    "        # 最终的1x1卷积层\n",
    "        self.final_conv = nn.Conv2d(out_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.encoder(x)\n",
    "        x2 = self.middle(x1)\n",
    "        x3 = self.decoder(x2)\n",
    "        return self.final_conv(x3)\n",
    "\n",
    "class TargetDetectionDataset(Dataset):\n",
    "    def __init__(self, img_folder, label_folder, transform=None):\n",
    "        self.img_folder = img_folder\n",
    "        self.label_folder = label_folder\n",
    "        self.transform = transform\n",
    "        self.img_paths = sorted(os.listdir(img_folder))\n",
    "        self.label_paths = sorted(os.listdir(label_folder))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_folder, self.img_paths[idx])\n",
    "        label_path = os.path.join(self.label_folder, self.label_paths[idx])\n",
    "\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        label = Image.open(label_path).convert('L')  # 灰度图像\n",
    "        \n",
    "        # 统一调整图像和标签的尺寸\n",
    "        img = img.resize((256, 256))\n",
    "        label = label.resize((256, 256))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            label = self.transform(label)\n",
    "        \n",
    "        # 标签二值化处理\n",
    "        label = (label > 0).float()  # 变为二值标签 (0或1)\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "# 数据转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 训练函数\n",
    "def train(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            imgs, labels = imgs.cuda(), labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)  # 移除unsqueeze(1)操作\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# 数据加载\n",
    "train_dataset = TargetDetectionDataset(\n",
    "    img_folder='img',\n",
    "    label_folder='label',\n",
    "    transform=transform\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# 初始化模型\n",
    "model = UNet(in_channels=3, out_channels=1).cuda()\n",
    "\n",
    "# 设置损失函数和优化器\n",
    "criterion = nn.BCEWithLogitsLoss()  # 二分类损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# 训练模型\n",
    "train(model, train_loader, criterion, optimizer, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate(model, img_path, output_path):\n",
    "    model.eval()\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_tensor = transform(img).unsqueeze(0).cuda()  # 加载并转换为tensor\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = model(img_tensor)\n",
    "        pred = torch.sigmoid(pred).squeeze().cpu().numpy()  # 使用Sigmoid将输出转化为0~1\n",
    "        pred = (pred > 0.5).astype(np.uint8) * 255  # 阈值处理\n",
    "\n",
    "    # 保存预测结果\n",
    "    pred_img = Image.fromarray(pred)\n",
    "    pred_img.save(output_path)\n",
    "\n",
    "# 示例：预测某张图像\n",
    "evaluate(model, 'mask/Hokkaido0001.tif', 'output_image.tif')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
